{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moovingsloth/video-attention-classification/blob/main/DAiSEE_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B_%EB%B6%84%EC%84%9D_%EA%B8%B0%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gcsfs tensorflow opencv-python tqdm --quiet\n",
        "\n",
        "import gcsfs\n",
        "from google.oauth2 import service_account\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "KEY_PATH = '/content/drive/MyDrive/keys/daisee-464310-abc81e54c985.json'\n",
        "SCOPES = ['https://www.googleapis.com/auth/cloud-platform']\n",
        "\n",
        "creds = service_account.Credentials.from_service_account_file(\n",
        "    KEY_PATH,\n",
        "    scopes=SCOPES\n",
        ")\n",
        "fs = gcsfs.GCSFileSystem(project='daisee-464310', token=creds)\n",
        "\n",
        "GCS_DATASET_DIR = 'gs://colab-daisee-bucket/DAiSEE/DataSet/'\n",
        "GCS_LABELS_DIR  = 'gs://colab-daisee-bucket/DAiSEE/Labels/'\n",
        "\n",
        "print(\"GCSFS ready. KEY_PATH:\", KEY_PATH)\n",
        "print(fs.ls('colab-daisee-bucket/DAiSEE/DataSet/'))\n",
        "print(fs.ls('colab-daisee-bucket/DAiSEE/Labels/'))"
      ],
      "metadata": {
        "id": "jBbTLbyqH8MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "batch_size = 32\n",
        "# Haar Cascade 알고리즘(사전 학습된(Pre-trained) 얼굴 검출 모델, xml 포맷)\n",
        "HAAR_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        "print(\"Using Haarcascade:\", HAAR_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ScR9DF-lzV8",
        "outputId": "5c4a8f4d-6e89-4d49-ea6f-4ce29c998350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Haarcascade: /usr/local/lib/python3.11/dist-packages/cv2/data/haarcascade_frontalface_default.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# 목표: 손실, 누락된 비디오 파일을 사전에 제거\n",
        "# 불완전한 프레임 건너뛰기 or 제거\n",
        "# 프레임 길이가 너무 짧거나 비정상인 크기 필터링\n",
        "# 타임윈도잉, 슬라이딩 윈도우 (LSTM, TCN 등에 넣을 고정된 시퀀스)\n",
        "# 불필요한 노이즈 제거(얼굴 검출, ROI)\n",
        "# 픽셀 레벨 정규화(normalization, equalizeHist())\n",
        "# Data Augmentation: 좌우뒤집기, 랜덤 크롭, 회전, 색변환 / 시간축 뒤집기\n",
        "# 클래스 불균형 처리: undersampling/oversampling / class_weight 조정\n",
        "# one-hot encoding, label smoothing\n",
        "# TFRecord/TF.Data 파이프라인 구성 - 캐싱, 셔플 등등\n",
        "\n",
        "class DataPreprocessing:\n",
        "    def __init__(self, fs, bucket: str, base_path: str = 'DAiSEE',\n",
        "                 IMG_HEIGHT: int = 224, IMG_WIDTH: int = 224,\n",
        "                 max_frames: int = 3, data_augmentation_flag: bool = False,\n",
        "                 num_workers: int = 8):\n",
        "        self.fs = fs\n",
        "        self.bucket = bucket\n",
        "        self.base_images = f'{bucket}/{base_path}/DataSet'\n",
        "        self.base_labels = f'{bucket}/{base_path}/Labels'\n",
        "        self.IMG_H, self.IMG_W = IMG_HEIGHT, IMG_WIDTH\n",
        "        self.max_frames = max_frames\n",
        "        self.data_augmentation_flag = data_augmentation_flag\n",
        "        self.num_workers = num_workers\n",
        "        # 얼굴 검출기 초기화\n",
        "        self.face_cascade = cv2.CascadeClassifier(\n",
        "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "        # 레이블 DataFrame 한 번만 로드\n",
        "        self.label_dfs = {}\n",
        "        for split in ('Train','Test','Validation'):\n",
        "            path = f'{self.base_labels}/{split}Labels.csv'\n",
        "            with self.fs.open(path,'rb') as f:\n",
        "                self.label_dfs[split] = pd.read_csv(f)\n",
        "\n",
        "    def get_video_paths(self, split: str) -> list:\n",
        "        pattern = f'{self.base_images}/{split}/*/*/*'\n",
        "        vids = [p for p in self.fs.glob(pattern) if p.lower().endswith(('.avi','.mp4'))]\n",
        "        print(f\"{split}: found {len(vids)} videos\")\n",
        "        return vids\n",
        "\n",
        "    def extract_frames(self, video_path: str) -> list:\n",
        "        # 임시 로컬 파일로 프레임 추출 (Thread-safe 작업)\n",
        "        tmp = '/tmp/tmp_video.avi'\n",
        "        with self.fs.open(video_path,'rb') as src, open(tmp,'wb') as dst:\n",
        "            dst.write(src.read())\n",
        "        cap = cv2.VideoCapture(tmp)\n",
        "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        indices = np.linspace(0, total-1, self.max_frames, dtype=int)\n",
        "        frames = []\n",
        "        for idx in indices:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            # BGR -> RGB 변환\n",
        "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "    def face_cropping(self, image: np.ndarray) -> np.ndarray:\n",
        "        # 얼굴 검출 및 크롭, 리사이즈\n",
        "        faces = self.face_cascade.detectMultiScale(image,1.3,5)\n",
        "        if len(faces)>0:\n",
        "            x,y,w,h = faces[0]\n",
        "            image = image[y:y+h, x:x+w]\n",
        "        return cv2.resize(image,(self.IMG_W,self.IMG_H), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    def augment_image(self, image: np.ndarray) -> list:\n",
        "        # 간단 증강 연산 정의\n",
        "        ops = [\n",
        "            lambda x: tf.image.flip_left_right(x).numpy(),\n",
        "            lambda x: tf.image.random_brightness(x,0.2).numpy(),\n",
        "            lambda x: tf.image.random_contrast(x,0.8,1.2).numpy()\n",
        "        ]\n",
        "        variants = []\n",
        "        for op in ops:\n",
        "            aug = op(image)\n",
        "            variants.append(cv2.resize(aug,(self.IMG_W,self.IMG_H)))\n",
        "        return variants\n",
        "\n",
        "    def _bytes_feature(self, value: bytes) -> tf.train.Feature:\n",
        "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "    def label_onehot(self, video_path: str, split: str) -> bytes:\n",
        "        # ClipID 기준 레이블 조회 및 one-hot 변환\n",
        "        vid = os.path.basename(video_path).rsplit('.',1)[0] + '.avi'\n",
        "        df = self.label_dfs[split]\n",
        "        row = df[df['ClipID']==vid]\n",
        "        if row.empty: return None\n",
        "        arr = row[['Boredom','Engagement','Confusion','Frustration ']].values[0]\n",
        "        return (arr>=1).astype(np.uint8).tobytes()\n",
        "\n",
        "    def _process_video(self, video_path: str, split: str) -> list:\n",
        "        # 비디오 단위 TFRecord Example 생성\n",
        "        lab = self.label_onehot(video_path, split)\n",
        "        if lab is None: return []\n",
        "        examples = []\n",
        "        frames = self.extract_frames(video_path)\n",
        "        for frame in frames:\n",
        "            img = self.face_cropping(frame)\n",
        "            variants = self.augment_image(img) if self.data_augmentation_flag else [img]\n",
        "            for v in variants:\n",
        "                jpeg = tf.io.encode_jpeg(v).numpy()\n",
        "                feat = {\n",
        "                    'image': self._bytes_feature(jpeg),\n",
        "                    'label': self._bytes_feature(lab)\n",
        "                }\n",
        "                examples.append(tf.train.Example(features=tf.train.Features(feature=feat)))\n",
        "        return examples\n",
        "\n",
        "    def writeTfRecord(self, output_dir: str = 'tfrecords'):\n",
        "        # Train/Test/Validation 동시 처리, 스레딩 적용\n",
        "        os.makedirs(output_dir,exist_ok=True)\n",
        "        for split in ('Train','Test','Validation'):\n",
        "            paths = self.get_video_paths(split)\n",
        "            writer = tf.io.TFRecordWriter(f'{output_dir}/{split.lower()}.tfrecords')\n",
        "            with ThreadPoolExecutor(max_workers=self.num_workers) as exe:\n",
        "                futures = {exe.submit(self._process_video, vp, split): vp for vp in paths}\n",
        "                for future in tqdm(as_completed(futures), total=len(paths), desc=split):\n",
        "                    for ex in future.result():\n",
        "                        writer.write(ex.SerializeToString())\n",
        "            writer.close()\n",
        "\n",
        "    def decode(self, example):\n",
        "        # TFRecord 파싱 및 디코딩\n",
        "        features = {\n",
        "            'image': tf.io.FixedLenFeature([],tf.string),\n",
        "            'label': tf.io.FixedLenFeature([],tf.string)\n",
        "        }\n",
        "        ex = tf.io.parse_single_example(example, features)\n",
        "        img = tf.image.decode_jpeg(ex['image'],channels=3)\n",
        "        lab = tf.io.decode_raw(ex['label'],tf.uint8)\n",
        "        return img, tf.cast(lab,tf.float32)"
      ],
      "metadata": {
        "id": "oWCkz0DPZtn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) TensorFlow 불러오기\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# 2) 배치 크기 설정\n",
        "batch_size = 32\n",
        "\n",
        "# 3) DataPreprocessing 인스턴스 생성\n",
        "dp = DataPreprocessing(\n",
        "    fs=fs,\n",
        "    bucket='colab-daisee-bucket',\n",
        "    max_frames=3,\n",
        "    data_augmentation_flag=True,  # 이 플래그로 증강 on/off 제어\n",
        "    num_workers=8\n",
        ")\n",
        "\n",
        "# 4) TFRecord 파일 생성\n",
        "print(\"Current working directory before writing TFRecords:\", os.getcwd())\n",
        "dp.writeTfRecord('tfrecords')  # writeTfRecord(output_dir)만 받습니다\n",
        "print(\"Current working directory after writing TFRecords:\", os.getcwd())\n",
        "\n",
        "\n",
        "# 5) TFRecordDataset + dp.decode + batch\n",
        "ds = tf.data.TFRecordDataset('tfrecords/train.tfrecords')\n",
        "ds = ds.map(dp.decode, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds = ds.batch(batch_size)\n",
        "\n",
        "# 6) 첫 배치 확인\n",
        "for imgs, labs in ds.take(1):\n",
        "    print('batch images:', imgs.shape, 'labels:', labs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "7hGPrMw-ZX7a",
        "outputId": "e63aba0f-177c-4b88-c0c9-a17428e6af93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'DataPreprocessing' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-2056483544.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 3) DataPreprocessing 인스턴스 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m dp = DataPreprocessing(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'colab-daisee-bucket'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataPreprocessing' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "시각화 및 레이블 분포(불균형 확인)\n",
        "\n",
        "메모리 캐시로 디스크 I/O 디코딩 비용 절감?\n",
        "'''\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1) TFRecord 파일 경로 설정\n",
        "tfrecords_dir = 'tfrecords'\n",
        "# Check if the directory exists in the current working directory, otherwise assume it's in /content/\n",
        "if not os.path.exists(tfrecords_dir):\n",
        "    tfrecords_dir = os.path.join('/content/', 'tfrecords')\n",
        "\n",
        "train_tfrecord = os.path.join(tfrecords_dir, 'train.tfrecords')\n",
        "val_tfrecord   = os.path.join(tfrecords_dir, 'validation.tfrecords')\n",
        "test_tfrecord  = os.path.join(tfrecords_dir, 'test.tfrecords')\n",
        "\n",
        "# 2) 레이블 디코딩 함수 (DataPreprocessing.decode와 동일)\n",
        "def decode(example_proto):\n",
        "    features = {\n",
        "        'image': tf.io.FixedLenFeature([], tf.string),\n",
        "        'label': tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    ex = tf.io.parse_single_example(example_proto, features)\n",
        "    img = tf.image.decode_jpeg(ex['image'], channels=3)\n",
        "    lab = tf.io.decode_raw(ex['label'], tf.uint8)\n",
        "    return img, tf.cast(lab, tf.float32)\n",
        "\n",
        "# 3) 데이터셋 읽어오기\n",
        "def make_dataset(tfrecord_path, batch_size=32):\n",
        "    ds = tf.data.TFRecordDataset(tfrecord_path)\n",
        "    ds = ds.map(decode, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_dataset(train_tfrecord, batch_size=32)\n",
        "val_ds   = make_dataset(val_tfrecord,   batch_size=32)\n",
        "test_ds  = make_dataset(test_tfrecord,  batch_size=32)\n",
        "\n",
        "# ——————————————————————————————\n",
        "# A) 샘플 시각화\n",
        "# ——————————————————————————————\n",
        "for imgs, labs in train_ds.take(1):\n",
        "    # 두 장만 꺼내서\n",
        "    for i in [0, 1]:\n",
        "        plt.figure(figsize=(4,4))\n",
        "        plt.imshow(imgs[i].numpy().astype(np.uint8))\n",
        "        plt.title(f\"Label vector: {labs[i].numpy()}\")\n",
        "        plt.axis('off')\n",
        "    break\n",
        "\n",
        "# ——————————————————————————————\n",
        "# B) 레이블 분포 확인\n",
        "# ——————————————————————————————\n",
        "def label_distribution(ds, name):\n",
        "    counts = np.zeros(4, dtype=int)\n",
        "    for _, labs in ds:\n",
        "        # labs: (batch, 4) one-hot 혹은 float 레이블\n",
        "        idx = np.argmax(labs.numpy(), axis=1)\n",
        "        binc = np.bincount(idx, minlength=4)\n",
        "        counts += binc\n",
        "    print(f\"{name} 분할 레이블 분포:\", counts)\n",
        "\n",
        "label_distribution(train_ds, \"Train\")\n",
        "label_distribution(val_ds,   \"Validation\")\n",
        "label_distribution(test_ds,  \"Test\")"
      ],
      "metadata": {
        "id": "U_Pt9XOelwRC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "24230731-bdcc-4fb8-9d8b-6afe6d6056ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} /content/tfrecords/train.tfrecords; No such file or directory [Op:IteratorGetNext] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-2128536088.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# A) 샘플 시각화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# ——————————————————————————————\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;31m# 두 장만 꺼내서\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6001\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6002\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} /content/tfrecords/train.tfrecords; No such file or directory [Op:IteratorGetNext] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ewV12PQUT_J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4a126e8",
        "outputId": "0c8bcc95-fab2-46c7-aaaa-fbf7cf67befb"
      },
      "source": [
        "import os\n",
        "\n",
        "print(\"Contents of root directory:\")\n",
        "print(os.listdir('/'))\n",
        "print(\"\\nContents of /content/ directory:\")\n",
        "print(os.listdir('/content/'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of root directory:\n",
            "['srv', 'root', 'proc', 'var', 'boot', 'opt', 'mnt', 'home', 'lib', 'run', 'lib32', 'etc', 'sbin', 'sys', 'usr', 'lib64', 'dev', 'libx32', 'bin', 'tmp', 'media', 'kaggle', '.dockerenv', 'datalab', 'tools', 'content', 'python-apt', 'python-apt.tar.xz', 'NGC-DL-CONTAINER-LICENSE', 'cuda-keyring_1.1-1_all.deb']\n",
            "\n",
            "Contents of /content/ directory:\n",
            "['.config', 'sample_data']\n"
          ]
        }
      ]
    }
  ]
}